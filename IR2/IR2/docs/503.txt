A Proposed Context-Awareness Taxonomy for Multi-data Fusion in Smart Environments: Types, Properties, and Challenges

    Authors
    Authors and affiliations

    Doaa Mohey El-DinEmail authorAboul Ella Hassanein Ehab E. Hassanien

    Doaa Mohey El-Din
        1Email author
    Aboul Ella Hassanein
        1
    Ehab E. Hassanein
        1

    1.Information Systems Department, Faculty of Computers and Artificial IntelligenceCairo UniversityCairoEgypt

Chapter
First Online: 27 June 2020

    303 Downloads 

Part of the Studies in Systems, Decision and Control book series (SSDC, volume 295)
Abstract

this paper presents a new taxonomy for the context-awareness problem in data fusion. It interprets fusing data extracted from multiple sensory datatypes like images, videos, or text. Any constructed smart environment generates big data with various datatypes which are extracted from multiple sensors. This big data requires to fuse with expert people due to the context-awareness problem. Each smart environment has specific characteristics, conditions, and roles that need to expert human in each context. The proposed taxonomy tries to cure this problem by focusing on three dimensions classes for data fusion, types of generated data, data properties as reduction or noisy data, or challenges. It neglects the context domain and introduces solutions for fusing big data through classes in the proposed taxonomy. This taxonomy is presented from studying sixty-six research papers in various types of fusion, and different properties of data fusion. This paper presents new research challenges of multi-data fusion.
Keywords
Data fusion Big data Telemedicine Internet-for-Things Smart environment Data visualization 
Access to this content is enabled by Egyptian Knowledge Bank
Download chapter PDF
1 Introduction

A smart environment is defined by the simulation of physical systems in the real system that is based on the connection between multiple sensors through the Internet. The extracted data has several characteristics: big volume, various formats types (such as text, video, audio, or image), and veracity of changes [1, 2]. It refers to a highly adaptive environment for various targets in sensor fusion [3]. The fusion process has happened when integrating the extracted data with various formats or various objectives. The fusion can interpret the data into useful values. Data fusion is classified further as low, intermediate and high corresponding to data-level, feature-level, and decision-level fusions [4, 5]. Researchers usually to provide and introduce a Multi-modal for sensor fusion to be more familiar for the human-computer interaction research field that used the input and delivering output involving multiple sensor modalities [6, 7].
The context-aware relies on the user‚Äôs targets and domain [8]. For any smart environment, Multi-sensor data fusion requires to answer the mention questions in the following:

    1.

    What is the target of the fusion?
     
    2.

    How many users can deal with the outcomes data?
     
    3.

    How can we make fusion?
     
    4.

    When can automate the fusion?
     
    5.

    Who gets the data from them?
     

Multi-model or Multi-sensor fusion still faces various challenges into fusion in different format types, meaningful interoperation, and specific domain [9, 10]. The adaptive context-aware relies on learning has problems in the accuracy rate, performance evaluation, and explanation reasoning of fusion. Multi-sensor fusion targets reaching the information heterogeneity of multi-sources. The raw data to analyze might be generated by a big number of heterogeneous sensors and extensive research effort has been devoted to coherently and accurately combine the resulting data.

The standardization of fusion-context types that is still the biggest problem in basic fusion is often targeted specific domain [11, 12, 13]. Any fusion model requires the expert user to follow the process of the fusion from scratch. However, the adaptive model of fusion targets getting benefits from the features to improve the fusion adaptation for multi-context.

This paper proposes a new standardization taxonomy for multiple types of context data fusion problem. The proposed taxonomy provides a new classification of context-awareness based on several dimensions: the type of context, the reduction process type, the data noisy, and the time of streaming this data. It presents a solution for the context-awareness problem by focusing on three dimensions classes for data fusion, types of generated data, data properties as reduction or noisy data, or challenges. It neglects the context domain and introduces solutions for fusing big data through classes in the proposed taxonomy.

The rest of the paper is organized as the following: Sect. 2, discusses the literature review of the context problem in data fusion, Sect. 3, the proposed taxonomy of data fusion, Sect. 4, Discussion, Sect. 5, open research challenges of data fusion and sensor fusion. Finally, Sect. 6 targets the conclusion outlines.
2 Literature Review

This section discusses the importance of multi-data fusion in smart environments. It also examines the context-awareness problem in data fusion and presents several taxonomies for fusion classification. Smart Environment refers the simulation systems for real environments based on connecting sensors via internet. It is based on internet-of-things and artificial intelligent. These data record each step in each second, that gets huge data with variant formats, various objectives or supplemental information with specific conditions and properties. Data fusion means the collecting big data from multiple sources to achieve the main target. The importance of data fusion is founded in the hardness in fusing huge data with various data properties or types. It improves making decisions and provides the accuracy results.

Context is defined by any data that can be utilized to depict the situation of an entity. An entity is defined by any object or people that interacts between a user and an application, including the user and applications themselves. The awareness is defined by the responsibilities of the system whether from users or conditions. Context-Awareness is still a big challenge of data fusion due to the specific domain properties, targets, and conditions. For each context domain, there are some data that consider complementary data for reaching the full vision of objective or idea. The main picture of context includes any factor affected on the environment that includes location, identity, environment, activities, and time. The context requires to inference the sensory data from various factors in one or various types.

Previous researches present several taxonomies for context-Awareness problem that are based on a study of context-aware frameworks. Researchers in [14], presents a new taxonomy based on eight categories to discuss the implementation criteria for implementing a context-aware system. These categories are system types, used sensors, context abstraction level, context model, context storage, architecture, privacy and security, and application domain. Researchers in [15], presents two main dimensions of context analysis in a proposed taxonomy for the mobile guides‚Äô literature. It creates multiple criteria for dealing with complexity of the mobile guides application space. It consists of five classifications for the mobile applications, context awareness system, user interface and requirements, social relationships, service tasks, and environment whether spatial or temporal. Researchers in [16], presents all data fusion taxonomies based on six dimensions that includes the relationship between multiple resources, the fusion abstraction level, the type of input and output, data level fusion, the data type fusion, user‚Äôs requirements in fusion process. Researchers in [17], presents a main data fusion taxonomy that is classified into five classes for improving the usability of data fusion systems. It includes five classes, relationships between applications, the level of abstraction for the input and the output, JDL data fusion framework, the architecture type of data fusion.
However, there is a main obstacle in all previous taxonomies that appears in how to deal with various context (as shown in Table 1). In other words, how can construct any fusion model without expert people. That requires the study of data, data types interpretation, features, and conditions check. So, there is a need to take at the occasional look of any applications. There are several conditions, features, and data changes that must consider in any data fusion construction application as the comparison in Table 2.
Table 1

A comparison between previous taxonomies of data fusion

Paper No.
	

Taxonomy target
	

Categories
	

Benefits
	

Limitations

[14]
	

Observation of the implementation systems of context-aware
	

Eight
	

Improving the implementation system methodology

Managing the fusion context control
	

Lack of context storage, less caring for data in any system

Lack of the criteria to secure data and secure the network

[15]
	

Introduce the taxonomy classification for mobile guides
	

Five
	

It can classify context to improve the mobile guides
	

Lack of technology evolution, lack of hard devices, less performance

[16]
	

Survey of all data fusion taxonomies
	

Six
	

Improving saving power, benefit from data redundancy. It discusses the previous data fusion taxonomies
	

No grantee spatial and temporal in fusion process concurrently

[17]
	

It classifies the proposed taxonomy based on applications strategies
	

Five
	

It proposes a data fusion taxonomy to classify fusion strategies to prevent ambiguously. Powerful taxonomy
	

Interpretation of fusion is highly complex to create fusion applications
Table 2

A comparison between data fusion applications

Paper No.
	

Context domain
	

Data size
	

Time stream type
	

Conditions
	

Data type
	

Advantages
	

Disadvantages

[19]
	

Telemedicine
	

1585 article for patients
	

Offline
	

Array of features of disease, cardiovascular disease (N‚Äâ=‚Äâ37) or diabetes (N‚Äâ=‚Äâ18)

aged ‚â•60 years
	

Audio or video
	

Monitoring remotely by 87%
	

There is hardness in adaption framework with several systems special national healthcare system

Can‚Äôt handle n 26 of 68 cases due to the lack of expert data for them

[20]
	

Daily living activities
	

Wearable dataset
	

Real-stream and offline
	

Automatic recognition of sedentary uses smart clothes (shirts)
	

Signal
	

Monitoring patients remotely by 95% accuracy results
	

Can‚Äôt interpret all activities. and requires authorized users for recording activities

[21]
	

Telehealth
	

Hand and fingerprint in 30 frames per second for five people in six times (Records 360)
	

Real-time
	

Color-based method for determination of hand location, and hand contour analysis
	

Image dataset
	

Improves for tracking and recognition system
	

Requires improving system by analyze contour for fingers and hands to be more flexible

[23]
	

Image fusion quality
	

Two different image types: anatomic (MRI) and functional (SPECT, positron emission tomography [PET])
	

Offline
	

Identifying tumor masses
	

Images
	

Good fusion for two images with more details
	

Takes a long time that requires improving the performance and taking less cost of time

[30]
	

Sonar videos
	

Sequenced data in 3 days in the underwater environments
	

Offline
	

Target tracking for images and record videos
	

Videos
	

Improving tracking
	

Requires tracking more scenarios to cover multiple cases

[31]
	

Video surveillance
	

Optical images
	

Real-time
	

Tracking objects to measure confidence and adoption
	

Images optical and infrared (IR) sensors
	

High accuracy
	

Finding localization errors and incorrect segmentation

[33]
	

Speech
	

Speech dataset
	

Offline
	

Diversity text-independent closed-set speaker identification
	

Speech
	

Improve performance 94.1%
	

Requires improving performance

[38]
	

Social media
	

Social media people‚Äôs reviews (getty imags, flick, twitter)
	

Offline (getty image: dataset containing more than 500,000 image and text pairs, for twitter 20,000 weakly, And Flickr 312,796 weakly)
	

Requires understanding English words, grammar, or keywords and comparing with the evaluation of images
	

Image-text
	

Improving prediction model for images and text)
	

Requires improving the effective due to the lack of datasets

[44]
	

Audio-visual data
	

Offline
	

Emotions samples of 830 frames in 6 emotions
	

Interpreting the semantic meaning of emotions in two data types
	

Emotion recognition for face and audio data
	

Fusing in semantic level for improving multi-model recognition, improving accuracy by 15% to reach 55.9%
	

Requires bigger dataset and improving accuracy results

[57]
	

Speech
	

7 males and 3 females
	

Offline
	

Measurements and simulation various SNR levels
	

Audio and visual
	

Improve decision fusion level and improve decision making with high accuracy
	

Improving results by some noisy data in testing

[71]
	

Outlier detection data
	

Has noisy data
	

Offline
	

Classifying outliers whether event or error
	

Video-audio-text
	

Increase accuracy by 14% by detecting outliers
	

Appling in various datasets and requires improving the performance

[77]
	

Multi context with several Attributes
	

Variant contexts
	

Real dataset for real sensors
	

Based on multi attitude theory
	

Sensors
	

Applies on variant contexts
	

Improving performance

In the following, a comparison between several data fusion applications in various contexts.

Previous comparison between twelve researches finds a big obstacle in variant context in context features, conditions, and properties (as shown in Table 2). Each environment requires creating a specific application for it, that becomes very hard in implementation and needs experiment people. All limitations in mention applications are shown in need choosing suitable size of a dataset, improving performance or accuracy results to be more effective or flexible system. The conclusion is no adaption fusion application for various contexts due to various datatypes, conditions and properties for each system.
3 The Proposed Taxonomy of Data Fusion

In recent years, a lot of attention has been paid to include context information in the data fusion process in order to reduce ambiguities and errors. The goals of fusion just only determine the type of fusion context. Our observation classifies the previous motivations that contain two types of categorization of fusion: based on the type of fusion and based on the type of data offline or real-time. These Categories targets reaching the semantic meaning of various contexts fusion.
The proposed taxonomy provides a new classification of context-awareness based on several dimensions: the type of context, the reduction process type, the data noisy, and the time of streaming this data, as shown in Fig. 1. A context type refers to four categories: one same type: text only, video, only, audio only, or image only, the second category: includes two context types only and the third contains three context types, and the fourth type: more than three types of context that has several motivations to be dynamic or adaptive recently.
Open image in new windowFig. 1
Fig. 1

The proposed taxonomy of classification of context-awareness modality
3.1 Fusion Type One: Based on A Context-Type Fusion
This type refers to the same type of context with different meanings such as several texts, several images and so on. This category depends on four Categorizations‚Äô classification as the following:

    1.

    Category one: The same Context Type:
     

    (1)

    Text-Context Fusion
     

There is a mutual idea between the ‚ÄòText‚Äô and ‚Äòcontext‚Äô, but the ‚ÄòContext‚Äô is more complex. The process of understanding the context and convert into text that refers to the semantic-pragmatic and right meaning. The text-context may be short words, numbers, or long documents. That is very useful for several research in machine translation to identify the culture filter dynamically for improving accuracy. The definition of the context often refers to everything around the text [17]. The fusion between various texts may cause of the new definition mentions entitled ‚Äòintertextuality‚Äô that text‚Äôs relation with other texts. The text usually focuses on the syntax of the sentence but the context and intertextuality focus on the semantic meaning of the text, in other words, the core of the text.

This type is very powerful in sentiment analysis when collecting the various sentiments via multi-sources about something. The telemedicine has also a portion of this area, there are motivations which cover the ECG, blood pressure about patients [18, 19, 20, 21]. These observation records in every second to manage remotely and medicine patients that can serve the patients in saving time, cost and their life‚Äôs. On another hand, supporting the doctors to observe a high number of patients at the same time and making decisions on their cases concurrently. Telemedicine becomes very important for saving people life‚Äôs and elderly people who stand alone.

    (2)

    Image-Context fusion
     

The image fusion of context includes some motivations such as flat image, 2 Dimensions, and 3 Dimensions, and X-rays images types [22, 23, 24, 25, 26, 27]. The image expresses the human emotion, disease type or analysis image, pattern identification such as fruits. All these fusion processes on different images use for enhancing the information quality. Not only has the type of image influenced the results but also the resolution of each image that can improve the classification. Colorized images, resolutions, pixels are factors that should identify from the first of the fusion process. That also can determine the anomaly detection of each fusion domain and environment [28]. X-ray and MRI images have several research to improve accuracy and classification [29]. That can support the Type and shape of tumors from the x-rays. There is another goal of fusion for recognizing the pattern classification that can support the size or weight of something.

Multiview fusion of images from the same modality and taken at the same time but from different viewpoints. Multimodal image fusion is a result of extracting data from multi-sensors (visible and infrared, CT and NMR, or panchromatic and multispectral satellite images) [30]. A multi-temporal fusion of images taken at different times in order to detect changes between them or to synthesize realistic images of objects which were not photographed in the desired time. Other types of image fusion focus on a 3D scene that picked repeatedly with several focal lengths. Fusion for image restoration, fusion two or more images of the same scene and modality, each of them blurred and noisy, may lead to a de-blurred and de-noised image. Multichannel deconvolution is a typical representative of this category.

    (3)

    Video-context fusion
     

There are a huge number of videos only online and offline, the new research targets to identify some patterns, emotions and actions in the video [31]. So, fusion can support various classification in the same time. That can serve the telemedicine of remotely monitoring patients and their actions in a real-time stream of television channels, news, Announcer, signer and actors online [19].

    (4)

    Audio-context fusion
     

Audio data fusion data refers to the audio data from various sensors [32]. Not only is it healthy, but it is also very important for people with disabilities. There are several motivations of research in converting the speech-to-text and still, the fusion between various audios is an obstacle of influencies modelo. The Speed, length, type of the audio are the essential three factors will impact on analyzing and fusing the data. They influence a Sequence of Overlapping Frames.

    2.

    Category two: based on two fusion context-types
     

    (1)

    Text-Speech
     

Previous research convert speech into text (Speech-to-text) to enhance pattern recognition and accuracy [33, 34]. The artificial intelligent algorithms provide several solutions for improving the accuracy of the meaning, pattern identification, and feature frequently. They often focus on the language‚Äôs grammar, verb, adverb, adjective or noun. This part-of-speech method can support the reliability of the system and enhance time performance. Lexicon-based is another method for text parsing and processing that can support by stop words list, each word has value and polarity. That has a good benefit for each domain individually. Machine learning [34] can classify several categorizations and predict the results. The recent researches [35, 36] benefit from the reliability of the deep learning algorithms to identify the features of text and enhance the accuracy of fusion. This type has some challenges into accent, language, requires translation or not, and convert usually the speech into text to unify the topology structure of variant context.

    (2)

    Text-Image
     

There are three types of image and text fusion,

    the image targets determining the text from it [37],

    the image should be expressed into text [38] to complete the fusion type or reversal situation [39].

    The image is an emotional feeling of sentiment opinion [40], the fusion mostly tries to convert text into an image to fuse images.

    (3)

    Text-Video
     

Researchers provide various solutions for identification of the text from the real-stream video which can improve the fusion levels and decide [40, 41, 42]. The main targets of their researchers are converting videos into storyboards. That requires to track information, determine the time, and a group of actions. That is considered multi-intelligent activities or actions which can detect some activates, and actions on them in spatiotemporal. The fusion process between video and text is proof in some applications, for example, the production of multimedia (e.g., video archiving and understanding). The challenge of this type often how to convert the actions in the video into interpreted text.

    (4)

    Video-Image
     

The motivations refer to split the streams of video into image frames. Although This fusion type is easier than previous fusion types, the performance time is very important to can classify, predict and fusion especially in real-stream of the videos. Motion estimation and warping functions are the main building blocks of the proposed framework [43]. The timestamp for each frame is very important for any video-image fusion. The researchers proposed [44] a software based on a hybrid distribution from Gaussian and uniform that was developed to be highly safe and reliable for the observations.

The main problem of this type demonstrates how to identify objects, activates into the video and interprets with the image meaning. Another question of this problem show in that is there any redundant data.

    (5)

    Video-Audio
     

While the audio is a major source of speech information, the visual component is a valuable supplementary information source in noisy environments because it remains unaffected by acoustic noise. Many studies have shown that the integration of audio and visual features leads to more accurate speaker identification even in noisy environments [45]. Audio-visual integration can be divided into three categories: feature fusion, decision fusion, and model fusion [46]. In feature fusion, multiple features are concatenated into a large feature vector and a single model is trained [47]. However, this type of fusion faces a problem in the hard representation of losing timing simultaneously for audio-visual features. In decision fusion, audio and visual features are processed separately to build two independent models, which completely ignore the audio-visual correlations. In model fusion, several models have been proposed, such as multi-stream hidden Markov model (HMM), factorial HMM, coupled HMM, mixed DBN, etc. [48, 49, 50, 51]. This type utilizes for tracking multiple speakers. Other research in this area targets determining the speech from the video. The authors aimed at detecting instances of aggressive human behavior in public environments based on Dynamic Bayesian Network [52]. The goal of visual-audio fusion improves the accuracy of video only or audio only and enhances the features‚Äô classification quality. There is a new approach to audio-visual fusion is focused on the modeling of audio and video signals. It targets decomposing each modality into a small set of functions representing the structures that are inherent in the signals. The audio signal is decomposed into a set of atoms representing concentrations of energy in the spectrogram (sounds) and the video signal is concisely represented by a set of image structures evolving through time, i.e. changing their location, size or orientation. As a result, meaningful features can be easily defined for each modality, as the presence of a sound and the movement of a salient image structure. Finally, the fusion step simply evaluates the co-occurrence of these relevant events. This approach is applied to the blind detection and separation of the audio-visual sources that are present in a scene.

In contrast, other methods presented used basic features and it is more focused on the fusion strategy that combines them. This approach was based on a nonlinear diffusion procedure that progressively erodes a video sequence and converts it into an audio-visual for the video sequence, where only the information that is required in applications in the joint audio-visual domain is kept. For this purpose, they define a diffusion coefficient that depends on the synchrony between video motions and audio energy and preserves regions moving coherently with the presence of sounds. Thus, the regions that are least diffused are likely to be part of the video modality of the audio-visual source, and the application of this fusion method to the unsupervised extraction of audio-visual objects is straightforward. The challenge of this type demonstrates how to convert and understanding the input videos and support the full idea of audios meanings. Another big obstacle faces the researchers here in the accent and language detection of the speech.

    3.

    Category Three: Three Fusion types
     

The main challenge of this category which type should select to convert all contexts into it. There is no standard model or method to support researchers in choosing suitable topology. So, this process takes a long time to search about the results of methods to check which converted topology that can hold the highest accuracy with good performance time. Most researchers make a great effort at searching for convenient algorithms to their objective. But they still face a big obstacle in selecting the suitable topology that can improve the classification, reduction, and pattern recognition.

In the following, a brief of several types of this type.

    (1)

    Text, audio, Image
     

The researchers presented a fusion method, based on deep neural networks, to predict personality traits from audio, language, and appearance [53]. They observed three modalities that included a signal relevant for personality prediction, that using all three modalities combined greatly outperforms using individual modalities, and that the channels interact with each other in a non-trivial fashion. By fusing the last network layers and fine-tuning the parameters we have obtained the best result, average among all traits, of 0.0938 Mean Square Error, which is 9.4% better in the performance. Video frames (appearance) are slightly more relevant than audio information (i.e. non-verbal parts of speech).

    (2)

    Text, Image, video
     

The evolution of this fusion type relies on the semantic meaning of the context. The keywords play a vital role to identify the domain and indexing the context‚Äôs features. The authors [54] introduced a probabilistic framework for semantic video indexing based on learning probabilistic multimedia representations of semantic events to represent keywords and key concepts. Ellis [55] presents a framework for detecting sources of sounds in audio using such cues as onset and offset. The proposed solution [56] to use machine learning in fusion between text, image and video contexts based on defining a dictionary for semantic concepts and using data retrieval. The authors [57] investigated the correlations between audio and visual features. A new audio-visual correlative model (AVCM) based on a dynamic Bayesian network (DBN) was proposed, which describes both the inter-correlations and the loose synchronicity between audio and visual streams. The experiments on the audio-visual bimodal speaker identification show that the AVCM model enhances the accuracy results.

    (3)

    Text, video, audio
     

The researchers presented a learning-based approach to the semantic indexing of multimedia content using cues derived from audio, visual, and text features. They used semantic labeling as a machine learning problem. They created a lexicon and used statistical methods for making the fusion between various contexts [58, 59]. A Proposed solution [58] is a middleware for context-aware applications that dynamically learns associative rules among context attributes. The main challenge of this type of how to reach the unified meaning of variant contexts according to each context feature.

    4.

    Category four: based on more than three types of fusion
     

This type faces the category three challenge also, which refers to the standardization of selecting the suitable topology to convert all various contexts into one topology automatically and dynamically with the highest accuracy.
Recently, the smart environment is a hot area of research and industry that includes huge data invariant topologies structures that require to analyze concurrently to support users in decision making in real-time. Any Smart environment has several sensors that hold the data about an object in one context type or more. Few motivations in this area to support some sensors or data fusion invariant context such as mobile sensors data. The recent researches target dynamic models or adaptive models to be convenient with some types of contexts [60, 61, 62]. The changing of the environment may be constant, but the system requires to be adaptive and dynamic [61] to each individual user and situation, that needs to automatic its behavior modifications simultaneously. Dynamic Bayesian networks (DBNs), in addition to current sensory readings, consider the past belief of the system, thus capturing the dynamicity of the phenomena under observation. This research used [62] to develop adaptive systems for getting automated inferences of the surrounding environment, the context-aware concept is adopted by the computing world in combination with data fusion. The research [63] provided a contribution tool that uses for data fusion based on features classification into twelve defect categories and using deep Convolutional Neural Networks (CNN) for surface defect detection in manufacturing processes. The researchers observed the effect on the result detection and compare between them as shown in Table 3.
Table 3

The observed comparison between data mining in the present methods and past ways
 	

Traditional mining
	

Recent mining

Domain
	

Single domain
	

Usually multi-domain

Volume of data
	

Small
	

Big

Modality
	

One context model
	

Multi-models of contexts

Datasets
	

Often unified
	

Diversity data

Distribution
	

Not different
	

Different

Data representation
	

Not different
	

Different

Data interconnection
	

Not requires
	

Paramount

Data noisy
	

Ignore it
	

Deal with outliers (error or event)

The evolution of context fusion in this type reaches creating a new adaptive model [64] for raw sensor data that can make extracting the data and features directly based on deep neural network and reinforcement learning. The researchers‚Äô approach [65] uses the convolutional neural networks (CNN) for improving the classification, features recognition with reliable performance but that requires to weakly supervised object localization. The experiment relies on the benchmark data set of PASCAL VOC 2007 and 2012 that demonstrates a new context-aware approach the significantly enhances weakly supervised localization and detection. The observation shows the comparison between data mining [66] past and present.

Recent researches [66, 67] discuss the gap in the extracted meaning from various contexts in internet-of-things (IoT) environments. Each environment has a target, domain and many objects which require to track them and know all activities on it with respect to the essential dimension known as ‚ÄòTime‚Äô. This dimension can support these environments in prediction the actions in the next time. The authors [67] introduced a new life cycle for the context in tracking objects and the design of contexts. That was considered a management framework for context-aware. But still, the tracking objects and objects management is a challenge that requires some motivation to improve accuracy and performance.
3.2 Fusion Type Two: Based on Data Reduction
The core of the difference between data integration [68, 69] and data fusion the data reduction [6, 70] process. The data integration targets to combine all data with others without neglect any data or avoid similar or complementary data. So, data integration requires making cleaning for integrated data and normalized. But the data fusion targets combine the data with some data or attributes reduction. Not any data can reduce or avoid it in fusion process. The fused data often interprets into three categories: data holds the same meaning, complementary data that includes some data in different attributes refer to the one objective and full idea of meaning, and the different data that can‚Äôt compensate instead of it (as shown in Fig. 2). The target of the fusion process that only can control which the data can apply the reduction of it. So not all the different data is important but there are some data related to the objective and user‚Äôs targets and some data not important that causes to headache on the fusion process.
Open image in new windowFig. 2
Fig. 2

The datatypes classification architecture
3.3 Fusion Type Three: With Respect to Noisy or Outliers

Anomalies, or outliers [71], can be a serious issue when training machine learning algorithms or applying statistical techniques. Errors and events cause of fault measurements and conditional reading that causes of faulty decisions. Outlier detection [72] refers to any strange phenomena with the same sample data. There are four types of outliers [73]: numeric, DBScan, Z-score and Isolation forest.
3.4 Fusion Type Four: Based on Data Time Stream Classification

    1.

    Type 1: Real-Time Stream
     

The recent motivations target the adaptive multi-model for context-aware but still have a big obstacle in the accuracy and performance measurements [74]. The essential challenge of how can make the data fusion process between variant topologies in real-time stream [75]. That requires huge data analysis and analytics simultaneously. This data analysis is very important in the decision making and saves lives in emergency cases in a smart environment such as smart city and smart health.

    2.

    Type 2: Offline data or near to real-time
     

Traditional mining researches of fusion targets offline analysis or near the real-time analysis. This process requires powerful computers and servers and good cloud and data integrity to guarantee no manipulation in the data from sensors.

Previous research targeted [76] constructing a context framework based on the hierarchical aggregation that deals with a broad spectrum of contexts, from personal (e.g., the activities of individuals) to city-wide (e.g., locations of groups of people and vehicles) and world-wide (e.g., global weather and financial data) [77]. Defined a formal model capable of representing context and situations of interest and developed a technique that exploits multi-attribute utility theory for fusing the modeled information and thereby attain situation awareness [78]. Smart Healthcare Applications and Services: Developments and Practices, an extensive framework to mediate ambiguous contexts in pervasive environments is presented in.

An extensive framework [79] to mediate ambiguous contexts in pervasive environments is presented in. Several works adopt [80] DBNs to perform adaptive data fusion for different applications, such as target tracking and identification and user presence detection [81].

It is a simulation of multiple-model [82] target tracking, data up-link, and missile state observer that used for terrain following navigation, transfer alignment, and pre-launch studies involving air-launched munitions from various fighter aircraft. Kalman Filter. A static schema of exclusion [83] of most costly sensors are often emery hungry that focus on the reduction of their consumption.

The proposed solution is constructed based on the occasional look of smart environment that for avoid the expert for each expectation. This solution targets to check properties, types, and challenges for multiple contexts. The proposed context system aims to construct the framework of any context with respect to data types, data properties, and various challenges. It is better than previous taxonomies [14, 15, 16, 17] in multiple context properties. The taxonomy in [14], requires discussing the abstraction level and architecture level. The taxonomy in [15], focuses on the complexity of context system. The taxonomy in [16], includes relationship between multiple resources and fusion levels. The taxonomy in [17], improves the usability of the fusion system. It differs in the properties construction system, better in neglecting the context properties. It prevents current the limitations of expert people for each domain, datatype formats, and context properties. The main measurements to select this taxonomy on how to fuse multiple sensory data with variant datatypes.
4 Discussion
Data fusion relies on fusing some multi-source with variant context types that holds some data mining processes that include the preprocessing data and identify patterns and requires visualizing these data (as shown in Fig. 3).
Open image in new windowFig. 3
Fig. 3

Data fusion processes
The basic architecture of data fusion [84] or sensor fusion taxonomy introduced three levels of fusion (as shown in Fig. 4).
Open image in new windowFig. 4
Fig. 4

The basic architecture of data fusion
The proposed data fusion taxonomy architecture shows in Fig. 1. That passes on all features of data fusion and mining processes as Fig. 3. The proposed architecture takes care of all affected features (inner and outer) on the data fusion process to reach the highest accuracy of fusion. The good fusion reaches good decision making, we can express it in a direct correlation as Eq. (1).
ùê∫ùëúùëúùëëùêπùë¢ùë†.‚àùùê∫ùëúùëúùëëùê∑ùëíùëê.ùëÄ,
(1)

The basic architecture still faces problems in understanding the level of data reduction, the outliers happen, and time streaming observation that effects on the analytics and users‚Äô requirements.

Our observation recommends using deep learning algorithms for improving accuracy and performance meaning because there are a big number of effected features in the smart environment. The smart environments require to decide concurrently and tracking several objects.
Each context-aware type in fusion process has some features that should consider in the fusion process at any fusion level (as shown in Table 4). A Context is a set of objects or characteristics or features for identifying simple situations or complex events. A ‚Äúcontext‚Äù has a necessary role at any level of Fusion. The fusion relies on two essential concepts, (1) Context is just collecting the characteristics of the data environment, (2) Context is a representation of physical environment features.
Table 4

The observed features of data fusion for context-aware types

Fusion context type
	

Features

Text
	

Detect language, syntax, semantic, and grammar

Audio
	

Speed, length, and type of speech

Image
	

Dimension type, resolution of the image, and image type

Video
	

Number of frames, real-stream or offline
The main challenge of all fusion types is the methodology or model that targets to combine several topology structures into one type. But other properties have a big effect in fusion in a smart environment that often in a real-time stream and have noisy. However, the level of reduction relies on the user‚Äôs requirements and the number of users known as Social Internet-of-Things as Eqs. (2) and (3).
ùëÖùëíùëëùë¢ùëê.ùêøùëíùë£ùëíùëô‚àùùëàùë†ùëíùëüùëÖùëíùëû,
(2)
ùëÅùëú.ùëàùë†ùëíùëüùë†‚àùùëÖùëíùëëùë¢ùëê.ùëôùëíùë£ùëíùëô,
(3)
The fusion taxonomy makes a full picture of the fusion properties in smart environments. They have become an integral part of the importance of the details of the fusion process (as shown in Fig. 5).
Open image in new windowFig. 5
Fig. 5

The summary of the proposed taxonomy
5 Open Challenges and Research Areas
The summary of the previously mentioned challenges (as shown in Fig. 6),
Open image in new windowFig. 6
Fig. 6

The challenges in standardization data fusion based on context types
5.1 A Standardization Challenge

The main standardization challenge is multi-model context-awareness data fusion. It depends on artificial intelligence, user interfaces. The fusion of context-aware requires the expert human in the specific domain. The previous categories refer to the types that can know the converted topologies automatically and the second category that refers to the types can‚Äôt know the converted topology automatically. There are two types of this challenge based on context: common challenges between the mentioned categories and different challenges between them as Fig. 6.
5.2 Expert Users

Each smart environment has one or more users that will be an expert to follow the fusion process and write features and thresholds of the proposed system. No way to know can automate the fusion with high accuracy and performance. Smart environment still faces a problem with user supervision and social Internet-of-things.

The user supervision refers to the expert user that can provide the system control. Social Internet-of-Things (SIoT) refers to the variant users‚Äô requirements that cause of making some fusion levels of data reduction.
5.3 Context Features Identification

Each context-aware type has several features but not all data have the features. There are some features missed and others must observe manually. The description of the data often is not enough to extract features of each context.
5.4 Choosing Topology

It is not an easy process to select the convenient topology that will convert into it such as (category 3, 4 in type 1 of the taxonomy). The recent researches have some types of conversions with a variant in the accuracy results. The comparison between results and algorithms that is not easy to choose the suitable topology for each fusion environment.
5.5 Data Integrity

The data integrity becomes a hot area of research that should guarantee data on the network. Measuring the quality of big data is significant to secure the data about sensors and objects. Data integrity is imposed within a system at its design stage using standard rules and procedures and is maintained using error checking and validation routines.
6 Conclusion and Future Work

Although data fusion is not a new field, the combination in the smart environment generates new requirements and challenges in data fusion. Smart environment relies on an interconnected set of sensors and objects that connected via internet that relies on the Internet-of-things domain. These sensors hold big data with variant contexts types. This paper demonstrates an observation classification taxonomy for data fusion that relies on types of contexts and some recent properties have a big effect on the smart environments. These properties area (data reduction, time streaming, and noisy data). This paper presents a new taxonomy for a context-awareness problem that is classified for multiple formats of data fusion. It introduces a new classification taxonomy based on classified types, properties, and challenges. The future work goes forwards to the new multi-model of context-awareness in any type to make a standardized model for reaching the convenient meaning automatically and identifying the features of data. This model targets automating classification, new features recognition and outlier detection. The purpose of the proposed application is to investigate proprieties from neuroscience providing insight into neural processing mechanisms involved in multisensory fusion for contextual awareness known to be present in humans.
References

    1.
    Thapliyal, R., Patel, R.K., Yadav, A.K., Singh, A.: Internet of Things for smart environment and integrated ecosystem. In: International Conference on Advanced Research in Engineering Science and Management At: Dehradun, Uttarakhand (2018)Google Scholar
    2.
    Bhayani, M., Patel, M., Bhatt, C.: Internet of Things (IoT): in a way. In: Proceedings of the International Congress on Information and Communication Technology, Advances in Intelligent Systems and Computing (2016)Google Scholar
    3.
    Bongartz, S., Jin, Y., Patern√≤, F., Rett, J., Santoro, C., Spano, L.D.: Adaptive User Interfaces for Smart Environments with the Support of Model-Based Languages. Springer, Berlin (2012)Google Scholar
    4.
    Ayed, S.B., Trichili, H., Alimi, A.M.: Data fusion architectures: a survey and comparison. In: 15th International Conference on Intelligent Systems Design and Applications (ISDA) (2015)Google Scholar
    5.
    Chao, W., Jishuang, Q., Zhi, L.: Data fusion, the core technology for future on-board data processing system. Pecora 15/Land Satellite Information IV/ISPRS Commission I/FIEOS 2002 Conference Proceedings (2002)Google Scholar
    6.
    Kalyan, L.O.: Veeramachaneni, Fusion, Decision-Level, Hindawi Publishing Corporation The Scientific World Journal Volume 2013, Article ID 704504, 19 pagesGoogle Scholar
    7.
    Lahat, D., Adal, T., Jutten, C.: Multimodal data fusion: an overview of methods, challenges and prospects. In: Proceedings OF THE IEEE (2015)Google Scholar
    8.
    Jaimes, A., Sebe, N.: Multimodal human computer interaction: a survey. Comput. Vis. Image Underst. 108(1), 116‚Äì134 (2007)CrossRefGoogle Scholar
    9.
    Kashevnika, A.M., Ponomareva, A.V., Smirnov, A.V.: A multi-model context-aware tourism recommendation service: approach and architecture. J. Comput. Syst. Sci. Int. 56(2), 245‚Äì258 (2017). (ISSN 1064-2307)CrossRefGoogle Scholar
    10.
    Lahat, D., Adali, T., Jutten, C.: Multimodal data fusion: an overview of methods, challenges, and prospects. Proc. IEEE 103(9) (2015)Google Scholar
    11.
    Hall, D.L., Llinas, J.: An introduction to multi-sensor data fusion. Proc. IEEE 85(1) (1997)Google Scholar
    12.
    Hofmann, M.A.: Challenges of model interoperation in military simulations. Simulation 80(12), 659‚Äì667 (2004)CrossRefGoogle Scholar
    13.
    El-Sappagh, S., Ali, F., Elmasri, S., Kim, K., Ali, A., Kwa, K.-S.: Mobile Health Technologies for Diabetes Mellitus: Current State and Future Challenges, pp. 2169‚Äì3536 (2018)Google Scholar
    14.
    ≈Ωontar, R., Heriƒçko, M., Rozman, I.: Taxonomy of context-aware systems. Elektrotehni≈°ki Vestnik 79(1‚Äì2), 41‚Äì46 (2012). (English Edition)Google Scholar
    15.
    Emmanouilidis, C., Koutsiamanis, R.-A., Tasidou, A.: Mobile guides: taxonomy of architectures, context awareness, technologies and applications. J. Netw. Comput. Appl. 36(1), 103‚Äì125 (2013)CrossRefGoogle Scholar
    16.
    Almasri, M., Elleithy, K.: Data fusion in WSNs: architecture, taxonomy, evaluation of techniques, and challenges. Int. J. Sci. Eng. Res. 6(4) (2015)Google Scholar
    17.
    Biancolillo, A., Boqu√©, R., Cocchi, M., Marini, F.: Data fusion strategies in food analysis (Chap. 10). In: Data Fusion Methodology and Applications, vol. 31, pp. 271‚Äì310 (2019)Google Scholar
    18.
    Ferrin, G., Snidaro, L., Foresti, G.L.: Contexts, co-texts and situations in fusion domain. In: 14th International Conference on Information Fusion Chicago, Illinois, USA (2011)Google Scholar
    19.
    den Berg, N., Schumann, M., Kraft, K., Hoffmann, W.: Telemedicine and telecare for older patients‚Äîa systematic review. Maturitas 73(2) (2012)Google Scholar
    20.
    Ka≈Ñtoch, E.: Recognition of sedentary behavior by machine learning analysis of wearable sensors during activities of daily living for telemedical assessment of cardiovascular risk. Sensors (2018)Google Scholar
    21.
    Kang, S.-K., Chung, K., Lee, J.-H.: Real-time tracking and recognition systems for interactive telemedicine health services. Wireless Pers. Commun. 79(4), 2611‚Äì2626 (2014)CrossRefGoogle Scholar
    22.
    Gite, S., Agrawal, H.: On context awareness for multisensor data fusion in IoT. In: Proceedings of the Second International Conference on Computer and Communication Technologies, pp. 85‚Äì93 (2015)Google Scholar
    23.
    Deshmukh, M., Bhosale, U.: Image fusion and image quality assessment of fused images. Int. J. Image Process. (IJIP) 4(5) (2010)Google Scholar
    24.
    Moravec, J., ≈†√°ra, R.: Robust maximum-likelihood on-line LiDAR-to-camera calibration monitoring and refinement. In: Kukelov√°, Z., SkovierovƒÉ, J.: (eds.) 23rd Computer Vision Winter Workshop, ƒåesk√Ω Krumlov, Czech Republic (2018)Google Scholar
    25.
    De Silva, V., Roche, J., Kondoz, A.: Robust fusion of LiDAR and wide-angle camera data for autonomous mobile robots. Sensors (2018)Google Scholar
    26.
    Ghassemian, H.: A review of remote sensing image fusion methods. Inf. Fusion 32(part A) (2016)Google Scholar
    27.
    Palsson, F., Sveinsson, J.R., Ulfarsson, M.O., Benediktsson, J.A.: Model-based fusion of multi- and hyperspectral images using PCA and wavelets. IEEE Trans. Geosci. Remote Sens. 53(5) (2015)Google Scholar
    28.
    Kim, Y.M., Theobalt, C., Diebel, J., Kosecka, J., Miscusik, B.: Sebastian, multi-view image and ToF sensor fusion for dense 3D reconstruction. In: IEEE 12th International Conference on Computer Vision Workshops, ICCV (2009)Google Scholar
    29.
    Choia, J., Radau, P., Xubc, R., Wright, G.A.: X-ray and magnetic resonance imaging fusion for cardiac resynchronization therapy. Med. Image Anal. 31 (2016)Google Scholar
    30.
    Krout, D.W., Okopal, G., Hanusa, E.: Video data and sonar data: real world data fusion example. In: 14th International Conference on Information Fusion (2011)Google Scholar
    31.
    Snidaro, L., Foresti, G.L., Niu, R., Varshney, P.K.: Sensor fusion for video surveillance. Electr. Eng. Comput. Sci. 108 (2004)Google Scholar
    32.
    Heracleous, P., Badin, P., Bailly, G., Hagita, N.: Exploiting multimodal data fusion in robust speech recognition. In: IEEE International Conference on Multimedia and Expo (2010)Google Scholar
    33.
    Boujelbene, S.Z., Mezghani, D.B.A., Ellouze, N.: General machine learning classifiers and data fusion schemes for efficient speaker recognition. Int. J. Comput. Sci. Emer. Technol. 2(2) (2011)Google Scholar
    34.
    Gu, Y., Li, X., Chen, S., Zhang, J., Marsic, I.: Speech intention classification with multimodal deep learning. Adv. Artif. Intell. (2017)Google Scholar
    35.
    Zahavy, T., Mannor, S., Magnani, A., Krishnan, A.: Is a picture worth a thousand words? A deep multi-modal fusion architecture for product classification in E-commerce. Under Review as a Conference Paper at ICLR 2017Google Scholar
    36.
    Gallo, I., Calefati, A., Nawaz, S., Janjua, M.K.: Image and encoded text fusion for multi-modal classification. Published in the Digital Image Computing: Techniques and Applications (DICTA), Australia (2018)Google Scholar
    37.
    Viswanathan, P., Venkata Krishna, P.: Text fusion watermarking in medical image with semi-reversible for secure transfer and authenticationGoogle Scholar
    38.
    Huang, F., Zhang, X., Zhao, Z., Xu, J., Li, Z.: Image-text sentiment analysis via deep multimodal attentive fusion. Knowl.-Based Syst. (2019)Google Scholar
    39.
    Blasch, E., Nagy, J., Aved, A., Pottenger, W.M., et al.: Context aided video-to-text information fusion. In: 17th International Conference on Information Fusion (FUSION) (2014)Google Scholar
    40.
    Video-to-Text Information Fusion Evaluation for Level 5 User Refinement,18th International Conference on Information Fusion Washington, DC, 6‚Äì9 July 2015Google Scholar
    41.
    Jain, S., Gonzalez, J.E.: Inter-BMV: Interpolation with Block Motion Vectors for Fast Semantic Segmentation on Video, arXiv:1810.04047v1
    42.
    Gidel, S., Blanc, C., Chateau, T., Checchin, P., Trassoudaine, L.: Non-parametric laser and video data fusion: application to pedestrian detection in urban environment. In: 12th International Conference on Information Fusion Seattle, WA, USA, 6‚Äì9 July 2009Google Scholar
    43.
    Katsaggelos, A.K., Bahaadini, S., Molina, R.: Audiovisual fusion: challenges and new approaches. Proc. IEEE 103(9) (2015)Google Scholar
    44.
    Datcu, D., Rothkrantz, L.J.M.: Semantic audio-visual data fusion for automatic emotion recognition, recognition. Emot. Recognit. 411‚Äì435 (2015)Google Scholar
    45.
    O‚ÄôConaire, C., O‚ÄôConnor, N.E., Smeaton, A.: Thermo-visual feature fusion for object tracking using multiple spatiogram trackers. Mach. Vis. Appl. 19(5‚Äì6), 483‚Äì494 (2008)Google Scholar
    46.
    Kumar, P., Gauba, H., Roy, P.P., Dogra, D.P.: Coupled HMM-based multi-sensor data fusion for sign language recognition. Pattern Recogn. Lett. 86 (2017)Google Scholar
    47.
    Chen, C., Liang, J., Zhao, H., Tian, J.: Factorial HMM and parallel HMM for gait recognition. IEEE Trans. Syst. Man Cybern. Part C (Appl. Rev.) 39(1), 114‚Äì123 (2009)CrossRefGoogle Scholar
    48.
    Cetin, O., Ostendorf, M. and Bernard, G.D.: Multi-rate coupled hidden markov models and their application to machining tool-wear classification. IEEE Trans. Signal Process. 55(6) (2007)Google Scholar
    49.
    Eyigoz, E., Gildea, D., Oflazer, K.: Multi-rate HMMs for word alignment. In: Proceedings of the Eighth Workshop on Statistical Machine Translation, Bulgaria, pp. 494‚Äì502 (2013)Google Scholar
    50.
    Zajdel, W., Krijnders, J.D., Andringa, T., Gavrila, D.M.: CASSANDRA: audio-video sensor fusion for aggression detection. In: IEEE International Conference Advanced Video and Signal Based Surveillance (AVSS), London, UK (2007)Google Scholar
    51.
    Kampman, O., Barezi, E.J., Bertero, D., Fung, P.: Investigating audio, video, and text fusion methods for end-to-end automatic personality prediction. In: Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pp. 606‚Äì611 (2018)Google Scholar
    52.
    Ji, C.B., Duan, G., Ma, H.Y., Zhang, L., Xu, H.Y.: Modeling of image, video and text fusion quality data packet system for aerospace complex products based on business intelligence (2019)Google Scholar
    53.
    Xiong, Y., Wang, D., Zhang, Y., Feng, S., Wang, G.: Multimodal data fusion in text-image heterogeneous graph for social media recommendation. In: International Conference on Web-Age Information Management WAIM, Web-Age Information Management (2014)Google Scholar
    54.
    Naphade, M., Kristjansson, T., Frey, B., Huang, T.S.: Probabilistic multimedia objects (multijects): a novel approach to 9 video indexing and retrieval in multimedia systems. In: Proceedings of IEEE International Conference on Image Processing, vol. 3, pp. 536‚Äì540, Chicago, USA (1998)Google Scholar
    55.
    Ellis, D.: Prediction-driven computational auditory scene analysis. Ph.D. thesis, MIT Department of Electrical Engineering and Computer Science, Cambridge, Mass, USA (1996)Google Scholar
    56.
    Adams, W.H., Iyengar, G., Lin, C.-Y., Naphade, M.R., Neti, C., Nock, H.J., Smith, J.R.: Semantic indexing of multimedia content using visual, audio, and text cues. EURASIP J. Appl. Signal Process. (2003)Google Scholar
    57.
    Wu, Z., Cai, L., Meng, H.: Multi-level fusion of audio and visual features for speaker identification. In: International Conference on Biometrics ICB 2006: Advances in Biometrics (2006)Google Scholar
    58.
    Yurur, O., Labrador, M., Moreno, W.: Adaptive and energy efficient context representation framework in mobile sensing. IEEE Trans. Mob. Comput. 13(8) (2014)Google Scholar
    59.
    De Paola, A., Gaglio, S., Re, G.L., Ortolani, M.: Multi-sensor fusion through adaptive Bayesian networks. Congress of the Italian Association for Artificial Intelligence AI*IA 2011: AI*IA 2011: Artificial Intelligence Around Man and Beyond (2011)Google Scholar
    60.
    Hossain, M.A., Atrey, P.K., El Saddik, A.: Learning multi-sensor confidence using a reward-and-punishment mechanism, integrate machine-learning algorithms in the data fusion process. IEEE Trans. Instrum. Meas. 58(5), 1525‚Äì1534 (2009)Google Scholar
    61.
    Gite, S., Agrawal, H.: On context awareness for multi-sensor data fusion in IoT. In: Proceedings of the Second International Conference on Computer and Communication Technologies (2016)Google Scholar
    62.
    Malandrakis, N., Iosif, E., Prokopi, V., Potamianos, A., Narayanan, S.: DeepPurple: lexical, string and affective feature fusion for sentence-level semantic similarity estimation. In: Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference, and the Shared Task. ACM (2013)Google Scholar
    63.
    Barzilay, R., McKeown, K.R.: Sentence fusion for multidocument news summarization. Comput. Linguist. 31(3) (2005)Google Scholar
    64.
    Durkan, C., Storkey, A., Edwards, H.: The context-aware learner. In: ICLR 2018Google Scholar
    65.
    Weimer Ariandy, D., Benggolo, Y., Freitag, M.: Context-aware deep convolutional neural networks for industrial inspection. In: Australasian Conference on Artificial Intelligence, Canberra, Australia, Volume: Deep Learning and its Applications in Vision and Robotics (Workshop) (2015)Google Scholar
    66.
    Brenon, A., Portet, F., Vacher, M.: Context feature learning through deep learning for adaptive context-aware decision making in the home. In: The 14th International Conference on Intelligent Environments, Rome, Italy (2018)Google Scholar
    67.
    Kantorov, V., Oquab, M., Cho, M., Laptev, I.: ContextLocNet: context-aware deep network models for weakly supervised localization. ECCV 2016, Oct 2016, Amsterdam, Netherlands. Springer, pp. 350‚Äì365 (2016)Google Scholar
    68.
    Savopol, F., Armenakis, C.: Merging of heterogeneous data for emergency mapping: data integration and data fusion? In: Symposium of Geospatial Theory, Processing and Applications (2002)Google Scholar
    69.
    Dong, X.L., Naumann, F.: Data fusion: resolving data conflicts for integration. J. Proc. VLDB 2(2) (2009)Google Scholar
    70.
    Zhu, Y., Song, E., Zhou, J., You, Z.: Optimal dimensionality reduction of sensor data in multisensor estimation fusion. IEEE Trans. Signal Process. 53(5) (2005)Google Scholar
    71.
    Nesa, N., Ghosh, T., Banerjee, I.: Outlier detection in sensed data using statistical learning models for IoT. In: 2018 IEEE Wireless Communications and Networking Conference (WCNC) (2018)Google Scholar
    72.
    Chandola, V., Banerjee, A., Kumar, V.: Outlier detection: a survey. ACM Comput. Surv. 41(3), Article 15 (2009)Google Scholar
    73.
    Aggarwal, C.C.: Outlier Analysis, 2nd edn. Springer, Berlin (2016)Google Scholar
    74.
    Tonjes, R., Ali, M.I., Barnaghi, P., Ganea, S., et al.: Real Time IoT Stream Processing and Large-scale Data Analytics for Smart City Applications (2014)Google Scholar
    75.
    Bonino, D., Rizzo, F., Pastrone, C., Soto, J.A.C., Ahlsen, M., Axling, M.: Block-based realtime big-data processing for smart cities. According to Eurostat, IEEE 2016Google Scholar
    76.
    Cho, K., Hwang, I., Kang, S., Kim, B., Lee, J., Lee, S., Park, S., Song, J., Rhee, Y.: HiCon: a hierarchical context monitoring and composition framework for next-generation context-aware services. IEEE Netw. 22(4) (2008)Google Scholar
    77.
    Padovitz, A., Loke, S.W., Zaslavsky, A., Burg, B., Bartolini, C.: An approach to data fusion for context awareness. In: International and Interdisciplinary Conference on Modeling and Using Context, Modeling and Using Context (2005)Google Scholar
    78.
    Roy, N., Das, S.K., Julien, C.: Resolving and mediating ambiguous contexts in pervasive environments. In: User-Driven Healthcare: Concepts, Methodologies, Tools, and Applications, IGI Global disseminator of knowledge (2013)Google Scholar
    79.
    Roy, N., Das, S.K., Julien, C..: Resource-optimized quality-assured ambiguous context mediation framework in pervasive environment. IEEE Trans. Mob. Comput. 11(2) (2012)Google Scholar
    80.
    De Paola, A., La Cascia, M., Lo Re, G., Ortolani, M.: User detection through multi-sensor fusion in an AmI scenario. In: 2012 15th International Conference on Information Fusion (FUSION) (2012)Google Scholar
    81.
    Roy, N., Pallapa, G.V., Das, S.K.: A middleware framework for ambiguous context mediation in smart healthcare application, user activity recognition. In: Third IEEE International Conference on Wireless and Mobile Computing, Networking and Communications, WiMob 2007, White Plains, New York, USA, 8‚Äì10 Oct 2007Google Scholar
    82.
    Nwe, M.S., Tun, H.M.: Implementation of multi-sensor data fusion algorithm. Int. J. Sens. Sens. Netw. (2017)Google Scholar
    83.
    Rahmati, A., Zhong, L.: Context-based network estimation for energy-efficient ubiquitous. IEEE Trans. Mob. Comput. 10(1) (2011)Google Scholar
    84.
    Klein, L., Mihaylova, L., El Faouzi, N.E: Sensor and data fusion: taxonomy challenges and applications. In: Pal, S.K., Petrosino, A., Maddalena, L. (eds.) Handbook on Soft Computing for Video Surveillance. Taylor & Francis. Sensor and Data Fusion: Taxonomy Challenges and applications. Chapman & Hall/CRC (2013)Google Scholar


